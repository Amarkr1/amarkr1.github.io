<!DOCTYPE html>
<!-- 
    Academic profile website, built entirely in Bootstrap 4
    Inspired by Academic theme for Hugo, see footer for GitHub reference link
    Free to use, modify, and distribute :)
    2021
-->
<html>
<nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top">
  <a class="navbar-brand" href="#">Amar Kumar</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNav">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link" href="#research">Education</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="#publications">Publications</a>
      </li>
    </ul>
  </div>
</nav>

<head>
    <meta charset="UTF-8">
    <title>Amar Kumar - Academic Profile</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/5.3.45/css/materialdesignicons.css" integrity="sha256-NAxhqDvtY0l4xn+YVa6WjAcmd94NNfttjNsDmNatFVc=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.1/css/all.min.css" integrity="sha256-2XFplPlrFClt0bIdPgpz8H7ojnk10H69xRqd9+uTShA=" crossorigin="anonymous" />
    <link rel="stylesheet" href="style.css"> 
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        h2.montserrat-font,
        h4.montserrat-font {
            font-family: montserrat,sans-serif;
            line-height: 1.25;
            text-rendering: optimizeLegibility;
            hyphens: manual;
            word-wrap: break-word;
            word-break: break-word;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="row">
        <div class="col-md-12 col-sm-12">
            <div class="row mt-4 ml-4 mr-4 mb-2">
                <div class="col-md-4 col-sm-12 text-center">
                    
                    <div class="pt-4 pb-4">
                        <div class="headshot-container">
                            <div class="headshot-frame">
                                <img alt="Profile Image" src="images/profile.jpg" class="headshot-image" />
                                <div class="headshot-overlay"></div>
                            </div>
                        </div>
                    </div>

                    <!-- Socials -->
                    <div class="row">
                        <div class="col-md-12 mt-1 mb-2 text-center">
                            <a href="https://github.com/Amarkr1"><i class="fab fa-github fa-3x"></i></a>
                            <a href="https://twitter.com/Amar_kumar_1"><i class="fab fa-twitter fa-3x"></i></a>
                            <a href="https://www.linkedin.com/in/amar-kumar-bb467775/"><i class="fab fa-linkedin fa-3x"></i></a>
                            <a href="https://scholar.google.ca/citations?user=mpaggKMAAAAJ&hl=en"><i class="ai ai-google-scholar ai-3x"></i></a>
                            <!-- <a href="#"><i class="ai ai-arxiv ai-3x"></i></a> -->
                        </div>
                    </div>
                    <!-- Title -->
                    <h2 class="montserrat-font">
                        Amar Kumar
                    </h2>
                    <!-- Profile Description -->
                    <h4 class="text-center text-muted mt-3 mb-5 montserrat-font" style="font-size:1.25rem">
                        Ph.D. Candidate, McGill University
                    </h4>
                    <!-- Mcgill logo -->
                    <div class="col-md-12 text-center">
                        <a href="https://www.mcgill.ca/">
                            <img alt="Mcgill logo" src="images/mcgill_logo_small.png" style="max-width:60%;height:auto" />
                        </a>
                    </div>
                    <!-- CIM Logo -->
                    <div class="col-md-12 text-center">
                        <a href="https://www.mcgill.ca/cim/">
                            <img alt="CIM logo" src="images/CIM_logo_2000px_RGB.png" class="img-fluid text-center" style="max-width:70%;height:auto" />
                        </a>
                    </div>
                    <!-- Mila Logo -->
                    <div class="col-md-12 text-center">
                        <a href="https://mila.quebec/en/">
                            <img alt="mila logo" src="images/mila.png" style="max-width:50%;height:auto" />
                        </a>
                    </div>
                </div>
                <div class="col-md-8 mt-5">
                    <div class="row">
                        <div class="col-md-5">
                            <!-- Title -->
                            <div class="page-header">
                                <h2 id="about">
                                    About
                                </h2>
                            </div>
                        </div>
                    </div>
                    <!-- Under title description -->
                    <p class="lead">
                    <br>I am a PhD student working under the supervision of Prof. Tal Arbel in the Probabilistic Vision Group (PVG).</br>
                    <br>My research primarily focuses on generative AI and medical imaging, with the main objective to tackle real-world challenges like bias mitigation in deep learning models. I also work on the explainability of deep learning and believe that the black-box nature of these models needs to be unraveled for improving trustworthiness and promoting the deployment of these methods in real-life applications. By understanding how these models make decisions, we can ensure they are fair and ethical in their outcomes. Additionally, I am interested in exploring how generative AI can be used to enhance medical imaging techniques, ultimately leading to more accurate diagnoses and personalized treatment for patients. </br>
                    </p>
                    
                    
                    
                    <!-- Research -->
                    <h4 class="mt-5" id="research">
                        <strong>Research Interests</strong>
                    </h4>
                    <!-- Modified research interests section with two columns -->
                    <div class="row">
                        <div class="col-md-6">
                            <ul class="lead">
                                <li class="list-item">
                                    Foundation Models
                                </li>
                                <li class="list-item">
                                    Generative Modeling
                                </li>
                                <li class="list-item">
                                    Fairness and Bias in Deep Learning 
                                </li>
                            </ul>
                        </div>
                        <div class="col-md-6">
                            <ul class="lead">
                                <li class="list-item">
                                    Explainable AI (XAI)
                                </li>
                                <li class="list-item">
                                    Medical Image Analysis
                                </li>
                                <li class="list-item">
                                    Computer Vision
                                </li>
                            </ul>
                        </div>
                    </div>
                    <div class="row mt-5">
                        <div class="col-md-6">
                            <!-- Education -->
                            <h4 id="education">
                                <strong>Education</strong>
                            </h4>
                            <ul class="list-unstyled fa-ul mt-3" style="margin-left:3.5rem">
                                <!-- One edu institution start -->
                                <li>
                                    <!--  <i class="fa-li fas fa-graduation-cap fa-2x"></i> -->
                                    <a href="https://www.mcgill.ca/"><img class="fa-li" style="left:-3em; top:0.4em" alt="mcgill logo mini" src="images/mcgill_mini.jpg"></a>
                                    <p class="lead" style="margin:0">PhD [Present]</p>
                                    <p class="text-muted">McGill University, Montreal, Canada</p>
                                </li>
                                <!-- One edu institution END -->

                                <!-- One edu institution start -->
                                <li>
                                    <!--  <i class="fa-li fas fa-graduation-cap fa-2x"></i> -->
                                    <a href="https://www.mcgill.ca/"><img class="fa-li" style="left:-3em; top:0.4em" alt="mcgill logo mini" src="images/mcgill_mini.jpg"></a>
                                    <p class="lead" style="margin:0">M.Eng. (Thesis) [2019]</p>
                                    <p class="text-muted">McGill University, Montreal, Canada</p>
                                </li>
                                <!-- One edu institution END -->

                                <!-- One edu institution start -->
                                <li>
                                    <!-- <i class="fa-li fas fa-graduation-cap fa-2x"></i> -->
                                    <a href="https://home.iitd.ac.in/"><img class="fa-li" style="left:-3em; top:0.4em" alt="iitd logo mini" src="images/iitd.png"></a>
                                    <p class="lead" style="margin:0">Bachelor of Technology (B.Tech) [2016]</p>
                                    <p class="text-muted">Indian Institute of Technology Delhi, Delhi, India</p>
                                </li>
                                <!-- One edu institution END -->
                                
                                <li>
                            </ul>    
                        </div>
                        <div class="col-md-6">
                            <!-- Experience -->
                            <h4 id="experience">
                                <strong>Experience</strong>
                            </h4>
                            <ul class="list-unstyled fa-ul mt-3" style="margin-left:3.5rem">
                                <!-- One experience item start -->
                                <li>
                                    <i class="fa-li fas fa-building fa-2x"></i>
                                    <p class="lead" style="margin:0">Senior Research Analyst</p>
                                    <p class="text-muted">Hedge Fund, India</p>
                                </li>
                                <!-- One experience item END -->
                                <!-- One experience item start -->
                                <li>
                                    <i class="fa-li fas fa-building fa-2x"></i>
                                    <p class="lead" style="margin:0">Research Intern</p>
                                    <p class="text-muted">University of Victoria, Canada</p>
                                </li>
                                <!-- One experience item END -->
                                <!-- One experience item start -->
                                <li>
                                    <i class="fa-li fas fa-building fa-2x"></i>
                                    <p class="lead" style="margin:0">Research Intern</p>
                                    <p class="text-muted">TIFR, India</p>
                                </li>
                                <!-- One experience item END -->
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Enhanced Publications Section with links and images opening in new tabs -->
            <div class="row mt-4 ml-4 mr-4 mb-4">
                <div class="col-md-12">
                    <hr class="mb-4" />
                    <div class="row">
                        <div class="col-md-4 mb-3">
                            <div class="publications-header text-right">
                                <h2 id="publications">
                                    Publications
                                </h2>
                            </div>
                        </div>
                        <div class="col-md-8">

                            

                            <!-- Publication Entry 1 -->
                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://openreview.net/forum?id=UpJMAlZNuo" class="link-primary text-decoration-none" target="_blank">PRISM: High-Resolution & Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion</a>
                                        </h4>
                                        <p class="text-dark mb-1"><strong>Amar Kumar</strong>, Anita Kriz, Mohammad Havaei, Tal Arbel</p>
                                        <p class="mb-1">Accepted to MIDL 2025</p>
                                        <p class="text-success font-weight-bold mb-2">Oral</p>
                                        <p class="mb-2">
                                            <a href="https://github.com/Amarkr1/PRISM" class="text-decoration-none" target="_blank"><i class="fab fa-github"></i> Source Code</a> | 
                                            <a href="https://amarkr1.github.io/PRISM" class="text-decoration-none" target="_blank"><i class="fas fa-globe"></i> Project Website</a> |
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-1"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-1" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at https://github.com/Amarkr1/PRISM.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            
                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://arxiv.org/abs/2503.23618v1" class="link-primary text-decoration-none" target="_blank">Leveraging Vision-Language Foundation Models to Reveal Hidden Image-Attribute Relationships in Medical Imaging</a>
                                        </h4>
                                        <p class="text-dark mb-1"><strong>Amar Kumar*</strong>, Anita Kriz*, Barak Pertzov, Tal Arbel</p>
                                        <p class="mb-1">Accepted to CVPR Workshop (MIV) Proceedings 2025 </p>
                                        <p class="mb-2">
                                            <a href="https://amarkr1.github.io/vlm4hiar" class="text-decoration-none" target="_blank"><i class="fas fa-globe"></i> Project Website</a> |
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-mivcausal"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-mivcausal" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    Vision-language foundation models (VLMs) have shown impressive performance in guiding image generation through text, with emerging applications in medical imaging. In this work, we are the first to investigate the question: 'Can fine-tuned foundation models help identify critical, and possibly unknown, data properties?' By evaluating our proposed method on a chest x-ray dataset, we show that these models can generate high-resolution, precisely edited images compared to methods that rely on Structural Causal Models (SCMs) according to numerous metrics. For the first time, we demonstrate that fine-tuned VLMs can reveal hidden data relationships that were previously obscured due to available metadata granularity and model capacity limitations. Our experiments demonstrate both the potential of these models to reveal underlying dataset properties while also exposing the limitations of fine-tuned VLMs for accurate image editing and susceptibility to biases and spurious correlation.                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://arxiv.org/abs/2503.23623" class="link-primary text-decoration-none" target="_blank">Language-Guided Trajectory Traversal in Disentangled Stable Diffusion Latent Space for Factorized Medical Image Generation</a>
                                        </h4>
                                        <p class="text-dark mb-1">Zahra TehraniNasab*, <strong>Amar Kumar*</strong>, Tal Arbel</p>
                                        <p class="mb-1">Accepted to CVPR Workshop (MIV) Proceedings 2025</p>
                                        <p class="mb-2">
                                            <a href="https://tehraninasab.github.io/sd-latent-traversal/" class="text-decoration-none" target="_blank"><i class="fas fa-globe"></i> Project Website</a> |
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-mivdis"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-mivdis" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    Text-to-image diffusion models have demonstrated a remarkable ability to generate photorealistic images from natural language prompts. These high-resolution, language-guided synthesized images are essential for the explainability of disease or exploring causal relationships. However, their potential for disentangling and controlling latent factors of variation in specialized domains like medical imaging remains under-explored. In this work, we present the first investigation of the power of pre-trained vision-language foundation models, once fine-tuned on medical image datasets, to perform latent disentanglement for factorized medical image generation and interpolation. Through extensive experiments on chest X-ray and skin datasets, we illustrate that fine-tuned, language-guided Stable Diffusion inherently learns to factorize key attributes for image generation, such as the patient's anatomical structures or disease diagnostic features. We devise a framework to identify, isolate, and manipulate key attributes through latent space trajectory traversal of generative models, facilitating precise control over medical image synthesis.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <!-- Publication Entry - RL4Med -->
                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://arxiv.org/abs/2503.15784" class="link-primary text-decoration-none" target="_blank">RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models</a>
                                        </h4>
                                        <p class="text-dark mb-1">Parham Saremi*, <strong>Amar Kumar*</strong>, Mohammed Mohammed, Zahra TehraniNasab, Tal Arbel</p>
                                        <p class="mb-1">In Review, 2025</p>
                                        <p class="mb-2">
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-rl4med"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-rl4med" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    Vision-Language Foundation Models (VLFM) have shown a tremendous increase in performance in terms of generating high-resolution, photorealistic natural images. While VLFMs show a rich understanding of semantic content across modalities, they often struggle with fine-grained alignment tasks that require precise correspondence between image regions and textual descriptions a limitation in medical imaging, where accurate localization and detection of clinical features are essential for diagnosis and analysis. To address this issue, we propose a multi-stage architecture where a pre-trained VLFM provides a cursory semantic understanding, while a reinforcement learning (RL) algorithm refines the alignment through an iterative process that optimizes for understanding semantic context. The reward signal is designed to align the semantic information of the text with synthesized images. We demonstrate the effectiveness of our method on a medical imaging skin dataset where the generated images exhibit improved generation quality and alignment with prompt over the fine-tuned Stable Diffusion. We also show that the synthesized samples could be used to improve disease classifier performance for underrepresented subgroups through augmentation.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <!-- Publication Entry 2 -->
                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://openreview.net/forum?id=M6CfJ5H7XH" class="link-primary text-decoration-none" target="_blank">DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based Counterfactual Explanations</a>
                                        </h4>
                                        <p class="text-dark mb-1">Nima Fathi*, <strong>Amar Kumar*</strong>, Brennan Nichyporuk, Mohammad Havaei, Tal Arbel</p>
                                        <p class="mb-1">Published at MIDL 2024</p>
                                        <p class="text-success font-weight-bold mb-2">Oral, Shortlist for the Best Paper Award</p>
                                        <p class="mb-2">
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-2"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-2" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions. Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved. In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability. Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class. The code is made publicly available at https://github.com/NimaFathi/DeCoDEx.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <!-- Publication Entry 3 -->
                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-45249-9_27" class="link-primary text-decoration-none" target="_blank">Debiasing Counterfactuals in the Presence of Spurious Correlations</a>
                                        </h4>
                                        <p class="text-dark mb-1"><strong>Amar Kumar</strong>, Nima Fathi, Raghav Mehta, Brennan Nichyporuk, Jean-Pierre R. Falet, Sotirios Tsaftaris, Tal Arbel </p>
                                        <p class="mb-1">Published at FAIMI, MICCAI 2023</p>
                                        <p class="text-success font-weight-bold mb-2">Best Oral Presentation Award</p>
                                        <p class="mb-2">
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-3"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-3" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <!-- Publication Entry 4 -->
                            <div class="publication-entry mb-4 p-3 bg-light rounded shadow-sm">
                                <div class="row">
                                    <div class="col-md-12">
                                        <h4>
                                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-19660-7_11" class="link-primary text-decoration-none" target="_blank">Counterfactual Image Synthesis for Discovery of Personalized Predictive Image Markers</a>
                                        </h4>
                                        <p class="text-dark mb-1"><strong>Amar Kumar</strong>, Anjun Hu, Brennan Nichyporuk, Jean-Pierre R. Falet, Douglas L. Arnold, Sotirios Tsaftaris, Tal Arbel </p>
                                        <p class="mb-1">Published at MIABID, MICCAI 2022</p>
                                        <p class="mb-2">
                                            <a href="#" class="text-decoration-none abstract-toggle" data-toggle="collapse" data-target="#abstract-4"><i class="fas fa-book-open"></i> Abstract</a>
                                        </p>
                                        <div id="abstract-4" class="collapse">
                                            <div class="card card-body">
                                                <p class="text-muted">
                                                    The discovery of patient-specific imaging markers that are predictive of future disease outcomes can help us better understand individual-level heterogeneity of disease evolution. In fact, deep learning models that can provide data-driven personalized markers are much more likely to be adopted in medical practice. In this work, we demonstrate that data-driven biomarker discovery can be achieved through a counterfactual synthesis process. We show how a deep conditional generative model can be used to perturb local imaging features in baseline images that are pertinent to subject-specific future disease evolution and result in a counterfactual image that is expected to have a different future outcome. Candidate biomarkers, therefore, result from examining the set of features that are perturbed in this process. Through several experiments on a large-scale, multi-scanner, multi-center multiple sclerosis (MS) clinical trial magnetic resonance imaging (MRI) dataset of relapsing-remitting (RRMS) patients, we demonstrate that our model produces counterfactuals with changes in imaging features that reflect established clinical markers predictive of future MRI lesional activity at the population level. Additional qualitative results illustrate that our model has the potential to discover novel and subject-specific predictive markers of future activity.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
    
        </div>
    </div>

    
    <!-- jQuery (required for bootstrap and collapse functionality) -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

    <!-- Navbar active state script -->
    <script>
      document.addEventListener('DOMContentLoaded', () => {
        const navLinks = document.querySelectorAll('.navbar-nav .nav-link');

        navLinks.forEach(link => {
          link.addEventListener('click', function () {
            // Remove active class from all links
            navLinks.forEach(nav => nav.parentElement.classList.remove('active'));

            // Add active class to the clicked link
            this.parentElement.classList.add('active');
          });
        });
      });
    </script>

    <!-- Abstract toggle script -->
    <script>
        $(document).ready(function() {
            $('.abstract-toggle').click(function(e) {
                e.preventDefault();
                $($(this).data('target')).collapse('toggle');
            });
        });
    </script>

    <!-- Visitor Counter Script -->
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const counterElement = document.getElementById('visitor-count');
        const workerBaseUrl = 'https://amarkr.amar-kumar.workers.dev';
        
        function updateCounterDisplay(count) {
            if (counterElement) {
                counterElement.textContent = count.toLocaleString();
            }
        }
        
        function handleError(error) {
            console.error('Error with visitor counter:', error);
            if (counterElement) {
                counterElement.textContent = '101';
            }
        }
        
        try {
            fetch(`${workerBaseUrl}/increment`)
                .then(response => {
                    if (!response.ok) {
                        throw new Error('Network response was not ok');
                    }
                    return response.json();
                })
                .then(data => {
                    updateCounterDisplay(data.count);
                })
                .catch(handleError);
        } catch (error) {
            handleError(error);
        }
    });
    </script>
</body>
</html>